{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25a7bd74",
   "metadata": {},
   "source": [
    "# Tropical Cyclones\n",
    "## Introduction\n",
    " \n",
    "We are now ready to consider the biggest beast of them all when it comes to weather/climate hazards. Over the next few sessions, we will be working on *tropical cyclones* -- namely, the risk they currently pose (or, rather, have historically posed), and how this may change under climate warming. \n",
    "\n",
    "The use of the term *risk* there was not accidental. We will be going beyond simpy assessing the hazard; we will be estimating impact as measured through total finanical losses (normalised to 2018 USD [$] to account for inflation). \n",
    "\n",
    "To work through these important concepts, we will be following a hypothetical scenario, outlined below, in which the US Treasury have hired you and your team as experts in tropical cyclone risk. \n",
    "\n",
    "### Scenario\n",
    "\n",
    "Your task is to summarise the potential financial impact of tropical cyclones over the next 100 years. They have asked you to assess: \n",
    "\n",
    "[1] The expected annual cost of tropical cyclones to the USA, assuming no further climate change. \n",
    "\n",
    "[2] The cost of a very expensive (i.e., rare) year -- again in a stationary climate. The Treasury define this as a year with costs exceeded in no more than 1 % % of years: the one-in-100-year cost. \n",
    "\n",
    "[3] How climate change may affect the risk\n",
    "\n",
    "You should address the requests of the Treasury by working through the steps in this Notebook, operating in pairs/teams if you desire (but no more than three to a group). \n",
    "\n",
    "### Data\n",
    "\n",
    "You will be working estimates of tropical cyclone damage from [ICAT](http://www.icatdamageestimator.com/viewdata). All costs are provided in US Dollars, adjusted for inflation to 2018 equaivalent.  \n",
    "\n",
    "When assessing the role of climate change, you will be using data from the [ERA5 reanalysis](https://www.ecmwf.int/en/forecasts/datasets/reanalysis-datasets/era5), combined with a [model](https://gmd.copernicus.org/articles/14/2351/2021/) to estimate the maximum potential intensity of tropical cyclones as a function of changes to sea surface temperature.  \n",
    "\n",
    "### Analysis\n",
    "\n",
    "Follow the workbook to address the requests of the Treasury. Remember to run code cells when you get to them by either pressing \"run\" at the of the browser window, or by hitting \"ctrl\" + \"enter\" on your keyboard. \n",
    "\n",
    "The first code cell (below) sets the notebook up for analysis. It also summarises the economic impacts from past hurricanes, printing the mean losses per year and per hurricane. The top ten years (by cost) are printed, and the most costly year is identified. Run this first code cell now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ac6214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and preliminaries\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "from scipy import stats\n",
    "from IPython.display import YouTubeVideo\n",
    "import xarray as xa, numpy as np\n",
    "from pyPI import pi\n",
    "import matplotlib.pyplot as plt\n",
    "from src import utils\n",
    "\n",
    "_nsim=10000\n",
    "data=pd.read_csv(\"Data/HurricaneDamage.csv\",index_col=\"lf_ISO_TIME\",parse_dates=True)\n",
    "data=data.loc[data[\"ND\"]>0] # throw away TCs that caused no damage\n",
    "data[\"ND\"]/=1e9 # Transform cost to billions\n",
    "anncost=data[\"ND\"].resample(\"Y\").sum()\n",
    "annidx=pd.date_range(start=\"%.0f/01/01\"%anncost.index.year[0],end=\"%.0f/01/01\"%anncost.index.year[-1],freq=\"1y\")\n",
    "anncost=anncost.reindex(annidx)\n",
    "ntc=len(data)\n",
    "ny=len(anncost)\n",
    "maxcost_storm=data.loc[data[\"ND\"]==data[\"ND\"].max()]\n",
    "maxcost_year=anncost.loc[anncost==anncost.max()]\n",
    "rate=ntc/ny # this is the rate for our TC occurrence model \n",
    "\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(\"%.0f damaging hurricanes in %.0f years\"%(ntc,ny))\n",
    "print(\"Rate = %.1f storms/year\"%rate)\n",
    "print(\"*** Cost summaries (amounts in billion, 2018 $ equivalent)***\")\n",
    "print(\"Mean annual cost is $%.1f\"%anncost.mean())\n",
    "print(\"Top ten years: \")\n",
    "for i in range(10):\n",
    "    print(\"\\t[%.0f] %.0f ($%.2f)\"%(i+1,(anncost.sort_values()[::-1]).index.year[i],\n",
    "          (anncost.sort_values()[::-1])[i]))\n",
    "print(\"Mean cost/storm = $%.2f\"%data[\"ND\"].mean())\n",
    "print(\"Most costly storm = %.0f ($%.2f)\"%(maxcost_storm.index.year[0],maxcost_storm[\"ND\"][0]))\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d62227f",
   "metadata": {},
   "source": [
    "Take a moment to appreciate the scale of the costs we're talking about here. For context, the cost of the Furlough scheme in the UK during the Covid-19 pandemic cost ~Â£70 billion (i.e., just under $100 billion). \n",
    "\n",
    "You may recall the devestating 2005 Hurricane season (in which Hurricane Katrina devestated New Orleans); and perhaps 2012 (when Hurricane Harvey generated unbelieveable rainfall totals in Houston). But perhaps 1926 -- and the Great Miami Hurricane -- is an event you have not yet heard of. Run the code below to find out more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c64bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A video explaining the most impactful hurricane you've never heard of. \n",
    "YouTubeVideo('LRAJ7Bc0O5E',width=800, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dd811c",
   "metadata": {},
   "source": [
    "## The expected value\n",
    "\n",
    "One way of estimating the expected cost is to compute the *sample* mean -- the 'average' of the annual costs. Using that approach, we already have an estimate of the expectation (in 2018 $) from the summary printed above. What is it?\n",
    "\n",
    "That was easy! But is it a useful answer? Possibly *not*, because as shown by the histogram below, *very* large values (i.e., expensive years) are *not* that unlikley, and that can make the sample mean -- of a relatively short dataset -- unrepresentative of the expected annual cost (i.e., the mean value we would observe if we could observe all *possible* years). \n",
    "\n",
    "Run the code to see the distribution of annual costs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232fb963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate histogram of the annual costs\n",
    "fig,ax=plt.subplots(1,1)\n",
    "h=ax.hist(anncost,bins=15,facecolor='white',edgecolor='k',linewidth=3)\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_xlabel(\"Cost ($2018 Billion)\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aeb48c",
   "metadata": {},
   "source": [
    "To put the magnitude of variability into perspective, the maximum annual bill (\\\\$265 billion) is not too far short of 100 times larger than the median value ($3 billion). If such variability existed in human heights, we should expect to encounter people well over 200 m tall! The possibility of such large \"events\" can make the mean *very* sensitive to sampling variability. To appreciate this, imagine calculating the mean height of all people in this room -- hoping it's a representative sample of all students on campus -- and then picture how that would change if a ~200 m tall student accidentally (tried) to pop in; your mean would go up -- a lot! \n",
    "\n",
    "We would be on a much firmer footing to compute the expected value if we knew the statistical distribution of the *population* (of heights, in this example). Knowing the population distribution is equivalent to being able to measure the heights of *all* students on campus. However, seldom can me measure all \"members\" of a population, so we instead make an assumption about the *probability distribution* (e.g., the Normal, GEV, etc.,) that represents the population, and use the properties of *that* to compute things like the expected value and, for eample, the probability of values beyond given thresholds (via the *CDF* -- as discussed in previous weeks).\n",
    "\n",
    "It would be very helpful, then, to compute the expected value from the probability distribution that represents *annual costs*. We'll get into that soon; first, let's deal with a slightly simpler question: what does the distribution of *costs per cyclone* look like?  \n",
    "\n",
    "\n",
    "## The distribution of cyclone impacts\n",
    "\n",
    "The reason why we're breaking things down like this is because, when you think on the processes for a little bit, it makes sense to deal with the damage per cyclone *seperately* from the number of cylones per year. The conceptual basis is explained in [Katz (2002)](https://journals.ametsoc.org/view/journals/apme/41/7/1520-0450_2002_041_0754_smohd_2.0.co_2.xml), but one intuitive reason to seperate them is because the annual cost can equal **exactly** zero (a year in which no damaging hurricanes make landfall), *or **any** other amount*, so our final statistical model must be capable of predicting exactly zero damages, *and* any other amount -- both with the right frequency. For this reason (and some others) it turns out separating our statistical approach into models (distributions) that deal with cyclone occurrence (number of storms per year), from those which consider impact (the cost per storm), is a good idea. So, with that explainer out the way, let's get back to assessing the distribution of costs per hurricane. Run the code to take a look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e0dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the fit for a log-normal distribution \n",
    "fig,ax=plt.subplots(1,2)\n",
    "fig.set_size_inches(8,3)\n",
    "h1=ax.flat[0].hist(anncost,bins=15,facecolor='white',edgecolor='k',linewidth=1.5,density=True)\n",
    "# Taking logs of the costs/storm, and then computing the mean and stdv of the logs\n",
    "lncost=np.log(data[\"ND\"]) # log transform\n",
    "lnmean=np.mean(lncost) # mean of log-transformed data\n",
    "lnstd=np.std(lncost) # stdv of log-transformed data\n",
    "x=np.linspace(np.min(lncost)-1,np.max(lncost)+2,100)\n",
    "ax.flat[1].plot(x,stats.norm.pdf(x, loc=lnmean,scale=lnstd),color='red')\n",
    "h2=ax.flat[1].hist(lncost,bins=15,density=True,facecolor='white',edgecolor='k',linewidth=1.5)\n",
    "ax.flat[0].grid(); ax.flat[1].grid()\n",
    "ax.flat[0].set_ylabel(\"Density\")\n",
    "ax.flat[0].set_xlabel(\"Cost ($2018 Billion)\")\n",
    "ax.flat[1].set_xlabel(\"ln{Cost ($2018 Billion)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0490bd",
   "metadata": {},
   "source": [
    "You will see in the plots that the distribution of costs/hurricane (left) looks very similar to the cost/year distribution: it has very strong positive skew, meaning values *much* greater than the median are not that unlikely. \n",
    "\n",
    "The ditribution on the right, however, looks very familiar. It seems to be very well described by the Normal distribution, as shown by the way the red line (the Normal pdf) matches the histogram well. The quantity plotted in this right-hand panel is the cost/storm series *after it has been log transformed*. \n",
    "\n",
    "*[Note that a log transform just means \"taking the logarithm\" of all values in the dataset; and the logarithm of a number is the exponent that a base must be raised to in order to equal that number. For example, if the base is 10 and the number is 100, the logarithm would be 2 (as 10$^{2}$=100). In our log transform we use the base e, which has a value of ~2.71]*\n",
    "\n",
    "The log transform has the effect of reducing larger numbers by greater amounts, and it results in our transformed data following a Normal distrubtion. Technically, this means that the underlying data follow what's called a *Log-Normal distribution*. This is great, it means that, just as we described extreme heat (in Pakistan) with the GEV distribution, it looks like we can use the Log-Normal distribution to describe the distribution of hurricane costs. \n",
    "\n",
    "The formula for the expected value (E[X]) from the Log-Normal distribution (given on pg. 88 of the [Wilks textbook](https://ebookcentral.proquest.com/lib/lboro/reader.action?docID=689817)) is: \n",
    "\n",
    "\\begin{aligned}\n",
    "E[X]=e^{\\mu + \\frac{\\sigma^{2}}{2}}\n",
    "\\end{aligned}\n",
    "\n",
    "where $\\sigma$ and $\\mu$ are the mean and standard deviation, respectively, of the *log-transformed* variable.\n",
    "\n",
    "The equation therefore tells us that to compute the expected value (i.e., the mean) for the cost/hurricane, we simply (a) log transform the data; (b) compute the mean and standard deviation of the transformed data; and (c) plug those values into the equation above. The code below does this. Run it to find out the expected cost per hurricane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eea90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lncost=np.log(data[\"ND\"]) # Log transform\n",
    "lnmean=np.mean(lncost) # Computing the mean of the log-transformed data\n",
    "lnstd=np.std(lncost) # Computing the standard deviation of the log-transformed data\n",
    "E=np.exp(lnmean+lnstd**2/2.)\n",
    "print(\"The expected value is $%.2f billion per hurricane\"%E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d169a13",
   "metadata": {},
   "source": [
    "How does this compare with the sample average computed at the beginning of this notebook?\n",
    "\n",
    "## The number of events per yar\n",
    "\n",
    "We now have a statistical model the cost per hurricane, but in order to estimate the expected cost per year, and the cost of a very expensive year (i.e., the one-in-200-year event), we need to also consider a statistical model for the *number of hurricanes per year*. \n",
    "\n",
    "What does this distribution look like? Run the code below to take a look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c7be49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the Poisson distribution\n",
    "# Compute the frequency with which ncylones/year are observed\n",
    "x=np.arange(0,10)\n",
    "of=np.array([np.sum(data.index.year==ii) for ii in anncost.index.year])\n",
    "off=np.array([np.sum(of==ii) for ii in x])\n",
    "gen=stats.poisson.rvs(rate,size=_nsim)\n",
    "sff=np.array([np.sum(gen==ii) for ii in x])/(_nsim/ny)\n",
    "\n",
    "fig,ax=plt.subplots(1,1)\n",
    "ax.bar(x,off,color='blue',alpha=0.8,label=\"Observed\")\n",
    "ax.bar(x,sff,color='red',alpha=0.25,label=\"Poisson\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xlabel(\"Hurricanes/year\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1effad30",
   "metadata": {},
   "source": [
    "You will again observe a distribution with positive skew, albeit much less pronounced than in the costs dataset. Although it's not a perfect match, you should additionally notice that the *Poisson* distribution is a reasonably good fit to our data. It succeeds in reproducing the basic shape of our histogram. \n",
    "\n",
    "As explained by [Katz (2002)](https://journals.ametsoc.org/view/journals/apme/41/7/1520-0450_2002_041_0754_smohd_2.0.co_2.xml), the Poisson distribution is also theoretically suited to describing the annual frequency of damaging hurricanes, so we will persist with its use here. \n",
    "\n",
    "Conveniently, the expected value (E[N]) for the Poisson distribution (i.e., the average number of damaging hurricanes per year) is simply the mean *observed* frequency, known as the \"rate\". We'll denote this quantity $\\lambda$. \n",
    "\n",
    "**What is the value for $\\lambda$ here? (Hint look at the stats printed out at the top of this notebook)**.\n",
    "\n",
    "Once we have determined $\\lambda$, we can compute the *expected cost per year* (E[Cy]) by combining the expected value for cost/hurricane E[Ch]) with the expected value of the number of hurricanes per year (E[N]):\n",
    "\n",
    "\\begin{aligned}\n",
    "E[Cy]=\\lambda e^{\\mu + \\frac{\\sigma^{2}}{2}}\n",
    "\\end{aligned}\n",
    "\n",
    "In other words, we just multiply the two expected values together! \n",
    "\n",
    "Run the code below to evaluate the expected cost below and print the number to screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18918745",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ecy=np.exp(np.mean(lncost)+lnstd**2/2)*rate\n",
    "print(\"The expected cost is $%.2f billion/year\"%Ecy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37c5fed",
   "metadata": {},
   "source": [
    "How does this compare to the sample average computed at the beginning of this notebook?\n",
    "\n",
    "## The magnitude of the one-in-100-year event\n",
    "\n",
    "The answer above addresses the first request of the Treasury. We must now turn our attention to their second question. How severe should we expect the costs of hurricane damage to be in a very *bad* year? \n",
    "\n",
    "This is harder to address with simple formuale, so we will approach the problem via a statistical *simulation*, where we generate a very long sequence of \"possible years\" that are statistically consistent with our observations. We then summarise the plausible scenario via simple descriptive statistics. \n",
    "\n",
    "The algorithm to implement this should be quite intuitive. For each year, we will:\n",
    "\n",
    "[1] Select a random number from the Posisson distribution to represent the number of damaging hurricanes in that year \n",
    "\n",
    "[2] Draw a random number from the Log-Normal distribution to represent its damage\n",
    "\n",
    "[3] Add up the damage from all hurricanes in the year to obtain the total damage. \n",
    "\n",
    "\n",
    "If we do this for many, many years (say 100,000), we should get a very good idea of what's likely for the 1-in-100 year event (because we will experience 1000 events as 'extreme' in the 100,000 year series). Computing the 99th percentile from the simulated timeseries provides us with the estimate of the 1-in-100 year event. \n",
    "\n",
    "When you have read (and understood) the above, run the code below to generate some plausible scenarios! \n",
    "\n",
    "Once the simulations are complete a timeseries plot showing all the plausible years is plotted, and the 99th percentile is printed to screen. For reference, we also highlight (with a red line) the most costly year in the observations (1926). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6664b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic simulation\n",
    "nsim=100000# This sets the number of years in our simulation\n",
    "nstorms=np.zeros(nsim,dtype=np.int)\n",
    "cost=np.zeros(nsim)\n",
    "for i in range(nsim):\n",
    "    nstorms[i]=int(stats.poisson.rvs(rate))\n",
    "    cost[i]=np.sum(np.exp(stats.norm.rvs(loc=lnmean,scale=lnstd,\n",
    "                                         size=nstorms[i])))\n",
    "fig,ax=plt.subplots(1,1)\n",
    "ax.plot(cost)\n",
    "ax.set_xlabel(\"Simulated year number\")\n",
    "ax.set_ylabel(\"Cost ($ Billion)\")\n",
    "ax.axhline(anncost.max(),color='red',label=\"1926 cost\")\n",
    "ax.set_ylim(0,np.max(cost))\n",
    "ax.legend()\n",
    "print(\" 99th percentile/1-in-100-year event would be $%.2f billion\"%np.percentile(cost,99))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06e1b64",
   "metadata": {},
   "source": [
    "You will all get slightly different answers for the above due the randomness of the simulation, but with a large enough value for nsim, I expect you are unlikley to differ by more than ~20 %. \n",
    "\n",
    "**Pause for thought**: How reaslistic, do you think the largest events in the simulated series are? If you judge them to be unreaslitic, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fe6bca",
   "metadata": {},
   "source": [
    "## Climate Change\n",
    "\n",
    "We are now ready to tackle the thorny issue of climate warming: how *may* the cost of tropical cyclone impacts change? The approach we will take here is simple: take the weather conditions observed during the very costly 2005 hurricane season, and then \"add on\" some climate warming -- an amount of our choosing -- to see what impact this could have on tropical cyclone maximum intensity. We compute maximum potential intensity using a [simple model](https://gmd.copernicus.org/articles/14/2351/2021/) that works out how powerful a cyclone could get, given the atmospheric state. Computing the % change in the 99th percentile of potential intensities (communicated in terms of maximum sustained wind speeds) -- between 2005 and our warmer climate -- enables use to estimate how strong tropical cyclones are likely to change with warming. \n",
    "\n",
    "How do we translate this increase in intensity to a change in economic impacts? A [study](https://iopscience.iop.org/article/10.1088/1748-9326/ab9be2) in the Western Pacific provides us with a simple rule of thumb: if wind speed increases by a factor of $x$, economic losses increase by a factor or $x^{1.70}$. We will use that here, but let's be clear: it is a rather crude approximation, and provides no more that a first approximation of potential impacts.\n",
    "\n",
    "So, to summarise, we work out how much intensity (the 99th percentile of maximum wind speed) is likely to change in a climate $n$ degrees warmer than 2005 (call this factor $x$); and we then compute $x^{1.70}$ to evaluate the increase in economic damages. For example, if $x$ = 1.25 for a climate state ($n$) = 2$^{\\circ}$C warmer than 2005, the increase in economic damage (relative to 2005) would be by a factor of 1.25$^{1.70}$ = 1.46 = 46%\n",
    "\n",
    "To perform this analysis, then, we must first compute the 99th percentile of maximum wind speed in the *reference* (2005) climate. Run the code below to do this; the result will print to screen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794d4729",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin=\"Data/era5_tc_ilev.nc\"\n",
    "data=xa.open_dataset(fin)\n",
    "data[\"msl\"]=data[\"msl\"]/100. #hPa\n",
    "nt,nlev,nr,nc=data[\"t\"].shape\n",
    "t=data[\"t\"].values\n",
    "q=data[\"q\"].values\n",
    "p=data[\"level\"].values\n",
    "sst=data[\"sst\"].values\n",
    "data[\"t\"]-=273.15\n",
    "data[\"sst\"]-=273.15\n",
    "result = xa.apply_ufunc(\n",
    "        pi,\n",
    "        data['sst'], data['msl'], data['level'], data['t'], data['q'],\n",
    "        kwargs=dict(CKCD=0.9, ascent_flag=0, diss_flag=1, ptop=50, miss_handle=1),\n",
    "        input_core_dims=[\n",
    "            [], [], ['level', ], ['level', ], ['level', ],\n",
    "        ],\n",
    "        output_core_dims=[\n",
    "            [], [], [], [], []\n",
    "        ],\n",
    "        vectorize=True\n",
    "    )\n",
    "vmax, pmin, ifl, t0, otl = result\n",
    "vmaxold=np.nanpercentile(vmax,99)\n",
    "print(\"The 99th percentile of maxumum potential intensity in 2005 is %.1fm/s\"%vmaxold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaa5013",
   "metadata": {},
   "source": [
    "With that baseline evaluated, it is now time to assess the potential impact of climate warming. \n",
    "\n",
    "We do that here by *specifying* an amount of warming of the sea-surface temperature *relative to 2005*. We also assume that air temperatures warm by the same amount, and that relative humidity remains unchanged. \n",
    "\n",
    "By setting the value of \"*dsst*\" in the code below, you control the amount of warming relative to 2005. Set this now in the code below; and then run the cell. The output will indicate the new 99th percentile of maximum potential intensity, plus the % change this represents relative to 2005; and finally the relative increase in hurricane costs (remember that this is equal to the relative change in intensity, raised to the power of 1.70). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285b3779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# * * * * * * * * * * * * *\n",
    "# C H A N G E    M E\n",
    "dsst=2\n",
    "# * * * * * * * * * * * * *\n",
    "\n",
    "dt=dsst*1.\n",
    "drh=0\n",
    "sst_out,t_out,rh_out,q_out=utils.gen_future_conditions(dsst,dt,drh,sst+273.15,t+273.15,q,p,nt,nlev,nr,nc)\n",
    "data[\"t\"].values=t_out-273.15\n",
    "data[\"sst\"].values=sst_out-273.15\n",
    "data[\"q\"].values=q_out\n",
    "result = xa.apply_ufunc(\n",
    "        pi,\n",
    "        data['sst'], data['msl'], data['level'], data['t'], data['q'],\n",
    "        kwargs=dict(CKCD=0.9, ascent_flag=0, diss_flag=1, ptop=50, miss_handle=1),\n",
    "        input_core_dims=[\n",
    "            [], [], ['level', ], ['level', ], ['level', ],\n",
    "        ],\n",
    "        output_core_dims=[\n",
    "            [], [], [], [], []\n",
    "        ],\n",
    "        vectorize=True\n",
    "    )\n",
    "vmax, pmin, ifl, t0, otl = result\n",
    "vmaxnew=np.nanpercentile(vmax,99)\n",
    "print(\"The 99th percentile is %.1fm/s\"%vmaxnew)\n",
    "factor=vmaxnew/vmaxold\n",
    "print(\"\\n\\nThe increase is therefore by a factor of %.2f\"%factor)\n",
    "increase=factor**1.70\n",
    "print(\"\\t...meaning that the increase in cost would be by a factor of %.2f\"%increase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107370e9",
   "metadata": {},
   "source": [
    "## Synthesis and questions\n",
    "\n",
    "We've covered a lot of ground here. It's now time to take stock. \n",
    "\n",
    "#### What did we do? \n",
    "Under a scenario of responding to a request from the US Treasury, we summarised the *observed* economic impacts from hurricanes making landfall in the USA. We then assessed the statistical distributions of the impact per hurricane (the Log-Normal), and the number of hurricanes per year (Poisson). We used a simple formula to compute the expected hurricane cost per year using paramaters from the the Log-Normal and Poisson distributions. To evaluate the expected magnitude of the 1-in-100-year hurricane season, we then used a \"*stochastic simulation*\" (i.e., the random sampling from the Poisson and Log-Normal distributions). The role of climate change was assessed by scaling impacts by an amount that depended on the increase in maximum potential intensity -- which we assessed by ramping up sea surface temperature (relative to 2005) and applying a simple model to estimate how this would affect hurricane winds. \n",
    "\n",
    "#### Questions  \n",
    "[1] Upon reading the report you prepared for the Treasury, the *US Secretary of Homeland Security* concludes: \n",
    "\n",
    "\"*Hurricanes are bad news. We know that and we've dealt with them just fine for over 100 years. We remember the awful 2005 season and already spend plenty on reducing vulnerability, so I see no reason for us rethink anything now.\"*\n",
    "\n",
    "How would you respond?\n",
    "\n",
    "[2] Next, you find that the press are very interested in your report. After interview you learn that a leading media outlet intends to frame the results with this headline: \n",
    "\n",
    "*Economic impacts from hurricanes to grow dramatically under climate change, study finds*\n",
    "\n",
    "If you had the opportunity to discuss with the editor before publication, what would you say?\n",
    "\n",
    "[3] Finally, which **three** recommendations would *you* have for the reduction of hurricane risk over the next 100 years?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4d3111",
   "metadata": {},
   "source": [
    "#### Further reading\n",
    "[Flyvbjerg (2020)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7533687/)\n",
    "\n",
    "[Malamud (2004)](https://iopscience.iop.org/article/10.1088/2058-7058/17/8/35/meta) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
